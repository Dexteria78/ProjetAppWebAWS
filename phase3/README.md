# Phase 3 - Haute DisponibilitÃ© et ScalabilitÃ©

## ğŸ“‹ Objectif

ImplÃ©menter une architecture hautement disponible et scalable avec un VPC dÃ©diÃ©, un Application Load Balancer et un Auto Scaling Group pour distribuer la charge sur plusieurs instances dans plusieurs zones de disponibilitÃ©.

## ğŸ¯ Exigences Phase 3

- âœ… Lancer un Ã©quilibreur de charge (ALB)
- âœ… CrÃ©er un Launch Template pour les instances EC2
- âœ… Mettre en place un Auto Scaling Group multi-AZ
- âœ… Tester l'application (affichage, ajout, suppression, modification)
- âœ… Effectuer un test de charge pour vÃ©rifier le scaling automatique

---

## ğŸ—ï¸ Architecture

```
                         Internet
                             â”‚
"ecr_repository_url" output {
                    â”‚ Internet Gateway â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
"ecr_repository_url" output {
              â”‚         VPC (10.0.0.0/16)   â”‚
              â”‚                             â”‚
  â”‚              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€
              â”‚  â”‚ Public   â”‚ â”‚ Public   â”‚  â”‚
              â”‚  â”‚ Subnet 1 â”‚ â”‚ Subnet 2 â”‚  â”‚
              â”‚  â”‚10.0.1.0  â”‚10.0.4.0 â”‚  â”‚  
              â”‚  â”‚(us-1a)   â”‚ â”‚(us-1b)   â”‚  â”‚
              â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚
              â”‚       â”‚   ALB       â”‚        â”‚
              â”‚       â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
              â”‚             â”‚                â”‚
              â”‚      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”         â”‚
              â”‚      â”‚Auto Scaling â”‚         â”‚
Group (2-5)  â”‚         â”‚              â”‚      
              â”‚      â”‚EC2 instancesâ”‚         â”‚
         â”‚              â”‚      â””â”€â”€â”€â”€â”€
              â”‚             â”‚                â”‚
     â”‚              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€
              â”‚  â”‚  Secrets Manager    â”‚     â”‚
     â”‚              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              â”‚             â”‚                â”‚
              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     
              â”‚  â”‚ Private  â”‚ Private  â”‚     â”‚
              â”‚  â”‚ Subnet 1 â”‚ Subnet 2 â”‚     â”‚
              â”‚  â”‚10.0.2.0  â”‚10.0.3.0  â”‚     â”‚
              â”‚  â”‚(us-1a)   â”‚(us-1b)   â”‚     â”‚
              â”‚  â”‚  RDS MySQL (db.t3)  â”‚     â”‚
              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
              â””â”€
```

## ğŸ“¦ Composants DÃ©ployÃ©s

### VPC & RÃ©seau
| Ressource | Valeur |
|-----------|--------|
| VPC CIDR | 10.0.0.0/16 |
| Public Subnet 1 | 10.0.1.0/24 (us-east-1a) |
| Public Subnet 2 | 10.0.4.0/24 (us-east-1b) |
| Private Subnet 1 | 10.0.2.0/24 (us-east-1a) - RDS |
| Private Subnet 2 | 10.0.3.0/24 (us-east-1b) - RDS |
| Internet Gateway | AccÃ¨s Internet pour subnets publics |

### Application Load Balancer
| ParamÃ¨tre | Valeur |
|-----------|--------|
| Type | Application (Layer 7) |
| Scheme | Internet-facing |
| Listener | HTTP port 80 |
| Health check path | `/` |
| Healthy threshold | 2 checks |
| Unhealthy threshold | 2 checks |
| Health check interval | 30s |
| Health check timeout | 5s |

### Auto Scaling Group
| ParamÃ¨tre | Valeur |
|-----------|--------|
| Min instances | 2 |
| Max instances | 5 |
| Desired capacity | 2 |
| AZ | us-east-1a + us-east-1b |
| Health check type | ELB |
| Health check grace period | 300s |

### Launch Template
| ParamÃ¨tre | Valeur |
|-----------|--------|
| AMI | Ubuntu 22.04 LTS (latest) |
| Instance type | t2.micro |
| Volume | 20 GB gp3 |
| IAM Profile | LabInstanceProfile |

### Scaling Policies (Target Tracking)
| Policy | Metric | Target |
|--------|--------|--------|
| CPU Tracking | ASGAverageCPUUtilization | 70% |
| Request Tracking | ALBRequestCountPerTarget | 1000 req/target |

### RDS MySQL
| ParamÃ¨tre | Valeur |
|-----------|--------|
| Engine | MySQL 8.0 |
| Instance | db.t3.micro |
| Storage | 20 GB gp3 |
| Multi-AZ | Non (coÃ»t) |
| Backup | DÃ©sactivÃ© (dÃ©veloppement) |
| Public access | Non |

### Autres Composants
- **AWS Secrets Manager** : `student-records-app-db-credentials-phase3`
- **AWS Cloud9** : Environnement t3.small pour administration (auto-stop 30 min)

## ğŸ”§ Choix Techniques

### VPC dÃ©diÃ© vs VPC par dÃ©faut
Phase 3 crÃ©e un **VPC dÃ©diÃ©** pour une isolation rÃ©seau complÃ¨te :
- ContrÃ´le total du plan d'adressage
- SÃ©paration publique/privÃ©e des ressources
- PrÃ©paration pour une architecture de production

### Dual Scaling Policies
Deux politiques de scaling complÃ©mentaires :
1. **CPU-based** : RÃ©agit Ã  la charge CPU des instances
2. **Request Count-based** : RÃ©agit directement au volume de requÃªtes HTTP

### EC2 dans subnets publics
Les instances EC2 sont dans les subnets publics pour simplifier le dÃ©ploiement (pas de NAT Gateway nÃ©cessaire), tout en Ã©tant protÃ©gÃ©es par Security Groups.

### RDS dans subnets privÃ©s
La base de donnÃ©es est entiÃ¨rement isolÃ©e dans des subnets privÃ©s sans accÃ¨s Internet.

## ğŸ“Š Fichiers Terraform

```
phase3/
 terraform.tf       # Provider AWS + version
 variables.tf       # Variables (CIDR, types, etc.)
 network.tf         # VPC, subnets, IGW, route tables
 security.tf        # Security Groups (ALB, web, RDS)
 loadbalancer.tf    # ALB, Target Group, Listener
 launch-template.tf # Template EC2 pour ASG
 autoscaling.tf     # ASG + scaling policies
 database.tf        # RDS MySQL + subnet group
 secrets.tf         # Secrets Manager
 app-secret.tf      # Secret pour l'application
 cloud9.tf          # Environnement Cloud9
 data.tf            # Data sources (AMI, IAM)
 outputs.tf         # Outputs (ALB URL, RDS endpoint...)
 userdata.sh        # Bootstrap EC2 instances
```

## 
```bash
# 1. Aller dans le dossier phase3
cd phase3

# 2. Initialiser Terraform
terraform init

# 3. VÃ©rifier le plan
terraform plan

# 4. Appliquer
terraform apply -auto-approve

# 5. RÃ©cupÃ©rer l'URL de l'application
terraform output application_url
```

### Temps de dÃ©ploiement estimÃ©
- VPC + rÃ©seau : ~30 secondes
- Security Groups : ~10 secondes
- ALB + Target Group : ~30 secondes
- RDS MySQL : ~8-10 minutes
- EC2 instances (via ASG) : ~3 minutes
- **Total : ~12-15 minutes**

## âœ… Tests EffectuÃ©s

### Test fonctionnel
```bash
# Page d'accueil
curl http://<ALB-DNS-NAME>/

# Liste des Ã©tudiants
curl http://<ALB-DNS-NAME>/students

# Ajout, modification, suppression via l'interface web
```

### Test de charge (loadtest)
```bash
npm install -g loadtest

# Test avec 100 utilisateurs simultanÃ©s pendant 60 secondes
loadtest -c 100 -t 60 http://<ALB-DNS-NAME>/students

# RÃ©sultats observÃ©s :
# - CPU monte au-dessus de 70%
# - ASG dÃ©clenche scale-out automatiquement
# - Nouvelles instances intÃ©grÃ©es dans ~3 minutes
# - CPU redescend, scale-in aprÃ¨s cooldown
```

## ğŸš§ DifficultÃ©s RencontrÃ©es

1. **Timing EC2 vs RDS**
   - **ProblÃ¨me** : Les instances EC2 dÃ©marraient avant que RDS soit disponible
   - **Solution** : `depends_on = [aws_db_instance.main, aws_secretsmanager_secret_version...]` dans l'ASG

2. **Health checks Ã©chouaient au dÃ©marrage**
   - **ProblÃ¨me** : L'application Node.js prenait du temps Ã  dÃ©marrer
   - **Solution** : Grace period de 300s + seuil de 2 checks seulement

3. **Subnets publics pour ASG**
   - **Observation** : Pas de NAT Gateway (coÃ»t), donc EC2 dans subnets publics
   - **Implication** : Les instances ont des IPs publiques mais sont protÃ©gÃ©es par SG

4. **CIDR overlap**
   - **ProblÃ¨me** : Chevauchement initial entre subnets publics et privÃ©s
   - **Solution** : Plan CIDR prÃ©cis (public: .1.x, .4.x / privÃ©: .2.x, .3.x)

## ğŸ“ˆ MÃ©triques

| MÃ©trique | Valeur |
|----------|--------|
| Latence moyenne (2 instances) | ~100ms |
| Throughput max (2 instances) | ~150 req/s |
| Temps scale-out | ~3 minutes |
| Min instances | 2 (haute disponibilitÃ©) |
| Max instances | 5 |

### CoÃ»ts estimÃ©s (us-east-1)
| Ressource | CoÃ»t/mois |
|-----------|-----------|
| ALB | ~$16 |
| EC2 2x t2.micro | ~$17 |
| RDS db.t3.micro | ~$12 |
| Data Transfer | ~$2 |
| Secrets Manager | ~$0.40 |
| Cloud9 (30min/j) | ~$1 |
| **Total** | **~$48/mois** |

## ğŸ”’ SÃ©curitÃ©

### Security Groups en cascade
```
Internet â†’ ALB-SG (port 80)
            â†“
         Web-SG (port 80 depuis ALB-SG uniquement)
            â†“
         RDS-SG (port 3306 depuis Web-SG uniquement)
```

### Isolation RDS
- Subnets privÃ©s, pas d'IP publique
- Accessible uniquement depuis les instances EC2
- Chiffrement possible au repos

### Secrets Manager
- Secret `student-records-app-db-credentials-phase3`
- JSON avec `username`, `password`, `host`, `dbname`
- RÃ©cupÃ©rÃ© dynamiquement au dÃ©marrage des instances

## ğŸ§¹ Nettoyage

```bash
terraform destroy -auto-approve
```

 Prochaines Ã‰tapes (Phase 4)## 

- Conteneurisation de l'application avec Docker
- Stockage de l'image sur Amazon ECR
- DÃ©ploiement du container sur EC2

## ğŸ”— RÃ©fÃ©rences

- [network.tf](network.tf) - VPC et subnets
- [loadbalancer.tf](loadbalancer.tf) - ALB configuration
- [autoscaling.tf](autoscaling.tf) - ASG et scaling policies
- [launch-template.tf](launch-template.tf) - Template EC2
- [database.tf](database.tf) - RDS MySQL
- [security.tf](security.tf) - Security Groups
- [userdata.sh](userdata.sh) - Bootstrap script
